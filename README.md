# Awsome-Knowledge-Graph-Reasoning 
# 2022 Papers
* *AAAI*:
[**TempoQR: Temporal Question Reasoning over Knowledge Graphs**](https://ojs.aaai.org/index.php/AAAI/article/view/20526/20285): TempoQR is a comprehensive embedding-based framework for answering complex questions involving temporal information over Temporal Knowledge Graphs (TKGs). It generates question-specific time embeddings and combines them with entity and context-aware information to ground the question to the specific entities and time scope it refers to. A transformer-based encoder then fuses this temporal information with the question representation for answer predictions. TempoQR was found to significantly improve accuracy and generalize better to unseen question types compared to state-of-the-art approaches.

[**Learning to Walk with Dual Agents for Knowledge Graph Reasoning**](https://ojs.aaai.org/index.php/AAAI/article/view/20538/20297): A dual-agent reinforcement learning framework has been developed to improve the accuracy and efficiency of graph walking for reasoning tasks over incomplete knowledge graphs. The approach trains two agents, Giant and Dwarf, to search for answers collaboratively, with Giant searching on cluster-level paths quickly and providing stage-wise hints for Dwarf. Experimental results on several KG reasoning benchmarks showed that the approach outperformed existing RL-based methods for long path queries by a large margin.

[**Reasoning about Causal Models with Infinitely Many Variables**](https://ojs.aaai.org/index.php/AAAI/article/view/20508/20267): Generalized structural equations models (GSEMs) are a generalization of structural equations models (SEMs) that can handle infinitely many variables with infinite ranges, which is useful for capturing dynamical systems. A sound and complete axiomatization of causal reasoning in GSEMs has been developed, which extends the sound and complete axiomatization provided by Halpern (2000) for SEMs. This axiomatization helps clarify the properties captured by Halpern's axioms.

[**KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning**](https://ojs.aaai.org/index.php/AAAI/article/view/16796/16603):KG-BART is a novel knowledge graph augmented pre-trained language generation model that incorporates the complex relations of concepts through a knowledge graph to produce more logical and natural sentences. It leverages graph attention to aggregate the rich concept semantics, improving generalization on unseen concept sets. Experiments on the CommonGen dataset show that KG-BART outperforms other pre-trained language generation models, particularly BART, in terms of BLEU scores. KG-BART's generated context can also be used as background scenarios to benefit downstream commonsense QA tasks.





# 2021 Papers

* [**SQALER: Scaling Question Answering by Decoupling Multi-Hop and Logical Reasoning**](https://papers.nips.cc/paper/2021/file/68bd22864919297c8c8a8c32378e89b4-Paper.pdf): A new approach to multi-hop reasoning over knowledge graphs (KGs) has been developed that scales linearly with the number of relation types in the graph, rather than the number of edges or nodes. This allows for more complex logical reasoning without losing expressive power and enables the solution of the multi-hop MetaQA dataset and achievement of a new state-of-the-art on the WebQuestionsSP dataset. The approach is orders of magnitude more scalable than competing methods and can achieve compositional generalization outside of the training distribution.

* [**Leveraging the Inductive Bias of Large Language Models for Abstract Textual Reasoning**](https://papers.nips.cc/paper/2021/file/8e08227323cd829e449559bb381484b7-Paper.pdf): Natural language models, such as GPT-3 and T5, have been found to provide useful inductive bias for training symbolic reasoning engines on nontraditional tasks. These engines demonstrate quick learning and natural generalization that reflects human intuition, such as the ability to generalize blockstacking to stacking other types of objects. The authors also found that compositional learning, where a learner trained on simpler tasks gains an advantage when tackling more complicated tasks, is effective in this context. The study focused on abstract textual reasoning tasks such as object manipulation and navigation, and demonstrated generalization to novel scenarios and symbols.

* [**SalKG: Learning From Knowledge Graph Explanations for Commonsense Reasoning**](https://proceedings.neurips.cc/paper/2021/file/9752d873fa71c19dc602bf2a0696f9b5-Paper.pdf): Saliency methods, which measure how much a knowledge graph (KG) feature influences the model to make a correct prediction, can be used to improve the performance of KG-augmented models on various commonsense reasoning tasks. This paper proposes a framework called SalKG, which uses coarse and fine saliency explanations created from a task's training set to guide the model's attention to useful KG features. SalKG was found to significantly improve performance on three popular commonsense QA benchmarks and a range of KG-augmented models, with up to a 2.76% absolute improvement on CSQA.

* [**ConE: Cone Embeddings for Multi-Hop Reasoning over Knowledge Graphs**](https://proceedings.neurips.cc/paper/2021/file/a0160709701140704575d499c997b6ca-Paper.pdf): A new technique called ConE has been developed to improve query embedding, which aims to represent entities and first-order logical queries in low-dimensional spaces in order to facilitate multi-hop reasoning over knowledge graphs. ConE is the first geometry-based model that can handle all first-order logical operations, including conjunction, disjunction, and negation. It represents entities and queries as Cartesian products of two-dimensional cones, with the intersection and union of cones representing conjunction and disjunction, and geometric complement operators in the embedding space representing negation. ConE was found to significantly outperform existing state-of-the-art methods on benchmark datasets.

* [**How to transfer algorithmic reasoning knowledge to learn new algorithms?**](https://papers.nips.cc/paper/2021/file/a2802cade04644083dcde1c8c483ed9a-Paper.pdf): Researchers have studied how to transfer algorithmic reasoning knowledge in order to solve tasks for which only input and output examples are available. They focused on two types of graph algorithms: parallel algorithms such as breadth-first search and Bellman-Ford, and sequential greedy algorithms like Prim and Dijkstra. They created a dataset including nine algorithms and three different graph types and found that standard transfer techniques were not sufficient to achieve systematic generalisation. Instead, they found that multi-task learning can be used to transfer algorithmic reasoning knowledge.

* [**Probabilistic Entity Representation Model for Reasoning over Knowledge Graphs**](https://papers.nips.cc/paper/2021/file/c4d2ce3f3ebb5393a77c33c0cd95dc93-Paper.pdf): A new technique called Probabilistic Entity Representation Model (PERM) has been developed to improve logical reasoning over Knowledge Graphs (KGs). The current approach uses spatial geometries such as boxes to learn query representations, but this has restrictive geometry and leads to non-smooth strict boundaries. PERM encodes entities as a Multivariate Gaussian density with mean and covariance parameters to capture its semantic position and smooth decision boundary, respectively. It also defines closed logical operations of projection, intersection, and union that can be aggregated using an end-to-end objective function. The technique was found to significantly outperform state-of-the-art methods on various benchmark KG datasets and was able to recommend drugs with better F1 scores in a COVID-19 drug-repurposing case study. The working of the query answering process was also demonstrated through a low-dimensional visualization of the Gaussian representations.

# 2020 Papers

* [**Beta Embeddings for Multi-Hop Logical Reasoning in Knowledge Graphs**](https://papers.nips.cc/paper/2020/file/e43739bba7cdb577e9e3e4e42447f5a5-Paper.pdf): Artificial intelligence researchers have developed a method called BetaE that can handle complex multi-hop logical reasoning over knowledge graphs (KGs). BetaE is the first approach that can handle all first-order logical operations, including conjunction, disjunction and negation, while also being able to model uncertainty. It uses probabilistic distributions with bounded support, specifically the Beta distribution, to embed queries and entities as distributions and performs logical operations in the embedding space using neural operators. BetaE was tested on three large, incomplete KGs and was found to increase relative performance by up to 25.4% over current state-of-the-art KG reasoning methods that can only handle conjunctive queries without negation.

* [**Duality-Induced Regularizer for Tensor Factorization Based Knowledge Graph Completion**](https://papers.nips.cc/paper/2020/file/f6185f0ef02dcaec414a3171cd01c697-Paper.pdf): A new regularizer called DURA has been developed to address the overfitting problem in tensor factorization-based models for knowledge graph completion. DURA is based on the observation that for an existing tensor factorization model, there is often another distance-based model closely associated with it. It is effective in improving the performance of existing models and widely applicable to various methods.
# 2019 Papers

* [DRUM: End-To-End Differentiable Rule Mining On Knowledge Graphs](https://papers.nips.cc/paper/2019/file/0c72cb7ee1512f800abe27823a792d03-Paper.pdf): DRUM is a scalable and differentiable approach for mining first-order logical rules from knowledge graphs for inductive link prediction. It is motivated by the connection between learning confidence scores for each rule and low-rank tensor approximation and uses bidirectional RNNs to share information across tasks. DRUM was found to be more efficient than existing rule mining methods for inductive link prediction on various benchmark datasets.

* 
